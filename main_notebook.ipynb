{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a672f35a",
   "metadata": {},
   "source": [
    "## 🔧 Setup and Imports\n",
    "Load required Python libraries for document processing, embedding, and vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30ece89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports: Core modules for RAG pipeline including transformer models, embedding utilities, and vector math\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import uuid \n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee771ac2",
   "metadata": {},
   "source": [
    "## 🔐 Environment Configuration\n",
    "Configure API keys securely using environment variables (e.g., for Gemini integration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c02381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemini API key from .env file for secure configuration\n",
    "load_dotenv()  # Load .env file\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fbd93",
   "metadata": {},
   "source": [
    "## 📄 Document Chunking\n",
    "Split long documents into token-length chunks to fit transformer model input limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b75eb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunking(directory_path: str, tokenizer, chunk_size: int, \n",
    "             para_separator: str = \"\\n\\n\", separator: str = \" \") -> Dict:\n",
    "    \"\"\"\n",
    "    Process documents in a directory and split them into chunks.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing documents\n",
    "        tokenizer: Tokenizer to measure chunk sizes\n",
    "        chunk_size: Maximum token size for each chunk\n",
    "        para_separator: Separator between paragraphs (default: \"\\n\\n\")\n",
    "        separator: Word separator (default: \" \")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all documents and their chunks\n",
    "    \"\"\"\n",
    "    documents = {}  \n",
    "    all_chunks = {}\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        print(f\"Processing: {filename}\")\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "            \n",
    "        # Get base filename without extension\n",
    "        base = os.path.basename(file_path)\n",
    "        sku = os.path.splitext(base)[0]\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        doc_id = str(uuid.uuid4())\n",
    "        document_chunks = {}  # Store chunks for this document\n",
    "        \n",
    "        # Split by paragraphs first\n",
    "        paragraphs = re.split(para_separator, text)\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            if not paragraph.strip():\n",
    "                continue\n",
    "                \n",
    "            # Split paragraph into words\n",
    "            words = paragraph.split(separator) \n",
    "            \n",
    "            current_chunk = []\n",
    "            current_chunk_str = \"\"\n",
    "            \n",
    "            for word in words:\n",
    "                test_chunk = f\"{current_chunk_str}{separator}{word}\" if current_chunk_str else word\n",
    "                \n",
    "                if len(tokenizer.tokenize(test_chunk)) <= chunk_size:\n",
    "                    current_chunk_str = test_chunk\n",
    "                else:\n",
    "                    if current_chunk_str:\n",
    "                        chunk_id = str(uuid.uuid4())\n",
    "                        document_chunks[chunk_id] = {\n",
    "                            \"text\": current_chunk_str,\n",
    "                            \"metadata\": {\"file_name\": sku}\n",
    "                        }\n",
    "                    current_chunk_str = word\n",
    "            \n",
    "            # \n",
    "            if current_chunk_str:\n",
    "                chunk_id = str(uuid.uuid4())\n",
    "                document_chunks[chunk_id] = {\n",
    "                    \"text\": current_chunk_str,\n",
    "                    \"metadata\": {\"file_name\": sku}\n",
    "                }\n",
    "        \n",
    "        documents[doc_id] = document_chunks\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d10cc7",
   "metadata": {},
   "source": [
    "## 🧠 Generate Document Embeddings\n",
    "Map each chunk of the document to its vector embedding using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4b3f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def map_document_embeddings(documents, tokenizer, model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function: map_document_embeddings\n",
    "    Maps all chunks of all documents to their respective embeddings using a given tokenizer and model.\n",
    "    Returns a dictionary of document/chunk keys to vector representations.\n",
    "    \"\"\"\n",
    "\n",
    "    mapped_document_db = {}\n",
    "    for doc_id, dict_content in documents.items():\n",
    "        mapped_embedding = {}\n",
    "        for content_id, text_content in dict_content.items():\n",
    "            text = text_content.get(\"text\")\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "            mapped_embedding[content_id] = embedding\n",
    "        mapped_document_db[doc_id] = mapped_embedding\n",
    "    return mapped_document_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6961d",
   "metadata": {},
   "source": [
    "## 🔍 Query-Based Retrieval\n",
    "Compare a query against all document chunks to retrieve the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f294fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_information(query, top_k, mapped_document_db):\n",
    "\n",
    "    \"\"\"\n",
    "    Function: retrieve_information\n",
    "    Computes similarity scores between query and all document embeddings to find the top-k most relevant chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_inputs = tokenizer(query, return_tensor='pt',padding=True, truncation=True)\n",
    "    query_embeddings = model(**query_inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "    query_embeddings = query_embeddings.tolist()\n",
    "    # converting query embeddings to numpy array\n",
    "    query_embeddings=np.array(query_embeddings)\n",
    "\n",
    "    scores = {}\n",
    "    for doc_id, chunk_dict in mapped_document_db.iteams():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "            chunk_embeddings = np.array(chunk_embeddings)\n",
    "\n",
    "\n",
    "            normalized_query = np.linalg.norm(query_embeddings)\n",
    "            normalized_chunk = np.linalg.morm(chunk_embeddings)\n",
    "\n",
    "            if normalized_chunk == 0 or normalized_query == 0:\n",
    "                score == 0\n",
    "            else:\n",
    "                score = np.dot(chunk_embeddings,query_embeddings)/ (normalized_chunk*normalized_query)\n",
    "            scores[(doc_id, chunk_id)] = score\n",
    "    sorted_scores = sorted(scores.items(),key=lambda item: item[1], reverse=True)[:top_k]\n",
    "\n",
    "    top_result = []\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        result = (doc_id,chunk_id, score)\n",
    "        top_result.append(result)\n",
    "    return top_result                   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94998ace",
   "metadata": {},
   "source": [
    "## 🧮 Compute Query Embedding\n",
    "Generate an embedding vector for a single user query using the same embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d864161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding(query, tokenizer, model):\n",
    "    \"\"\"Compute embedding for a single query\"\"\"\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**query_inputs)\n",
    "        query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return query_embedding.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedea949",
   "metadata": {},
   "source": [
    "## 📊 Cosine Similarity Calculation\n",
    "Measure semantic similarity between query and document chunks using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e736d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity_score(query_embeddings, chunk_embeddings):\n",
    "\n",
    "    \"\"\"\n",
    "Function: calculate_cosine_similarity_score\n",
    "Calculates cosine similarity between query and each document chunk.\n",
    "Returns similarity scores in a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized_query = np.linalg.norm(query_embeddings)\n",
    "    normalized_chunk = np.linalg.norm(chunk_embeddings)\n",
    "\n",
    "    if normalized_chunk == 0 or normalized_query ==0:\n",
    "        score == 0\n",
    "    else:\n",
    "         score = np.dot(chunk_embeddings, query_embeddings)/ (normalized_chunk * normalized_query)  \n",
    "         \n",
    "    return score\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef6a28",
   "metadata": {},
   "source": [
    "## 🏆 Top-K Scoring and Filtering\n",
    "Sort chunks by similarity score and extract the top-k most relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4afc3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k):\n",
    "\n",
    "    \"\"\"\n",
    "Function: retrieve_top_k_scores\n",
    "Sorts document chunks by similarity to the query and selects top-k results.\n",
    "    \"\"\"\n",
    "     \n",
    "    scores = {}\n",
    "    for doc_id, chunk_dict in mapped_document_db.items():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "            chunk_embeddings = np.array(chunk_embeddings) \n",
    "            score = calculate_cosine_similarity_score(query_embeddings,chunk_embeddings)\n",
    "            scores[(doc_id, chunk_id )] = score\n",
    "        sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]   \n",
    "    return sorted_scores      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39daaa95",
   "metadata": {},
   "source": [
    "## 📦 Format Retrieved Results\n",
    "Organize and present retrieved chunks from top scores into a final response structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f529348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_results(sorted_scores):\n",
    "\n",
    "    \"\"\"\n",
    "🔹 Function: retrieve_top_results\n",
    "Extr\n",
    "    \"\"\"\n",
    "    top_results=[]\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        results = (doc_id, chunk_id, score)\n",
    "        top_results.append(results)\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a9cfe",
   "metadata": {},
   "source": [
    "## 💾 Save Results to Disk\n",
    "Utility function to store retrieved results or document metadata in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1905232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(path,data):\n",
    "    \"\"\"\n",
    "🔹 Function: save_json\n",
    "Utility function to write results or metadata to disk in JSON format.\n",
    "\"\"\"\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67d32cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4bd0343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_text(top_results, document_data):\n",
    "    first_match = top_results[0]\n",
    "    doc_id = first_match[0]\n",
    "    chunk_id = first_match[1]\n",
    "    related_text = document_data[doc_id][chunk_id]\n",
    "    return related_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1149796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_response(gemini_model, query, relevent_text):\n",
    "\n",
    "    template = \"\"\"\n",
    "    \n",
    "    You are an intelligent search engine. you will be provided with some retrieved contexts, as well as the user query.\n",
    "\n",
    "    Your jon is to understand the request, and answer based on the retrieved context.\n",
    "    Here is context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "    chain = prompt | gemini_model\n",
    "    response=chain.invoke({\"context\":relavent_text[\"text\"],\"question\":query})\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5aa5fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Document processing setup\n",
    "    directory_path = \"documents\"\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Text processing parameters\n",
    "    chunk_size = 200\n",
    "    para_separator = \"\\n\\n\" \n",
    "    separator = \" \"\n",
    "    top_k = 2\n",
    "    \n",
    "    # Gemini AI setup (corrected usage)\n",
    "    genai.configure(api_key=\"GEMINI_API_KEY\")  \n",
    "    gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f14882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: behaviuor1.txt\n",
      "Processing: behaviuor2.txt\n",
      "Processing: behaviuor3.txt\n"
     ]
    }
   ],
   "source": [
    "#creating document store with chunk id, doc_id, text\n",
    "documents = chunking(directory_path, tokenizer, chunk_size, para_separator, separator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79406635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now embedding generation and mapping in database\n",
    "mapped_document_db = map_document_embeddings(documents, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a5cf27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving json\n",
    "save_json('database/doc_store_2.json', documents) \n",
    "save_json('database/vector_store_2.json', mapped_document_db) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60b4ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Retrieving most relavent data chunks\n",
    "query = \"What are effective strategies to prevent tantrums in toddlers, especially when they can't express themselves well?\"\n",
    "query_embeddings =  compute_embedding(query, tokenizer, model)\n",
    "sorted_scores = retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k)\n",
    "top_results = retrieve_top_results(sorted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0caa1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading json\n",
    "document_data = read_json(\"database/doc_store_2.json\") #read document store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a767e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Retrieving text of relavent chunk embeddings\n",
    "relavent_text = retrieve_text(top_results, document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a452c0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Tantrums are common during the second year of life, when language skills are developing. Because toddlers can\\'t always say what they want or need, and because words describing feelings are more complicated and develop later, a frustrating experience may cause a tantrum. As language skills improve, tantrums tend to decrease.\\nToddlers want independence and control over their environment — more than they can actually handle. This can lead to power struggles as a child thinks \"I can do it myself\" or \"I want it, give it to me.\" When kids discover that they can\\'t do it and can\\'t have everything they want, they may have a tantrum.\\nHow Can We Avoid Tantrums?\\nTry to prevent tantrums from happening in the first place, whenever possible. Here are some ideas that may help:\\n    • Give plenty of positive attention. Get in the habit of catching your child being good. Reward your little one with praise and attention for positive behavior. Be', 'metadata': {'file_name': 'behaviuor1'}}\n"
     ]
    }
   ],
   "source": [
    "print(relavent_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
